# Flux Usernetes

We are going to test deploying usernetes with Flux and [Lima](https://lima-vm.io). Instead of having different VMs to start a separate control plane and workers, we are going to have several (identical) VMs that are running Flux, one with a lead broker, and have them connect (to make a Flux instance on a single cluster) and then launch usernetes in the job. Will it work? I have no idea. Let's gooo! For installation instructions of lima see [this top level README.md](../README.md). Note that you also need the special mount type! In the steps below we will:

 - [Setup](#setup) a shared volume with a flux configuration file (TBA will be written into the VM startup)
 - Nodes
   - [Manual Usernetes](#manual-usernetes): setup Usernetes and Flux separately and run usernetes inside a flux broker instance (done).

Next we will do the same setup, but:

- Write the broker.toml in the VM build (it doesn't have anything changing in it)
- Do not setup usernetes manually, but instead in a batch job.

Note that for the VM builds it doesn't make it to the end, and I'm not sure why. I think I'm just bad with the probes? But the log reports exit code 0 and everything I expect is built, so likely me just being a noob.

## Setup

Copy the broker.toml into /tmp/lima

```bash
cp broker.toml /tmp/lima/broker.toml
```

After creating the nodes you'll need to update the hostnames of each in /etc/hosts to refer to the corresponding hostname, e.g.,

```console
192.168.104.6  lima-flux-0
192.168.104.7  lima-flux-1
```

## Nodes

### Manual Usernetes

Let's manually create two nodes to start (you can run these in separate terminals)

```bash
limactl start --network=lima:user-v2 --name=flux-0 ./flux-usernetes.yaml
limactl start --network=lima:user-v2 --name=flux-1 ./flux-usernetes.yaml
```

The "user-v2" refers to the network, which you can find in `~/.lima/_config/networks.yaml`.
We can now shell in and start flux or finish the usernetes control plane.
This will eventually be done by flux, but not yet.
Note that typically usernetes would be in a home, but we aren't binding or creating one here.

```bash
# Change your home here to be your {username}.linux
limactl shell --workdir /home/vanessa.linux/usernetes flux-0
```

If you do this manually, flux-0 needs:

```bash
make up
make kubeadm-init
make install-flannel
make kubeconfig
export KUBECONFIG$HOME/kubeconfig
make join-command
echo "export KUBECONFIG=$HOME/kubeconfig" >> ~/.bashrc
```

And flux-1 needs:

```bash
# Change your home here to be your {username}.linux
limactl shell --workdir /home/vanessa.linux/usernetes flux-1
make -C ~/usernetes up kubeadm-join
```

### Manual Flux

Let's start trying to manually create a flux instance that connects the two VMs. The `/tmp/lima/broker.toml` references flux-lima-[0-1] so you'll need to get ip addresses for each vm with `ip a` and then add to the `/etc/hosts` of each VM:

```console
192.168.104.6  lima-flux-0
192.168.104.7  lima-flux-1
```

#### Lead Broker

Then shell into the flux-0

```bash
limactl shell --workdir /home/vanessa.linux/usernetes flux-0
```

And start the lead broker.

```bash
sudo chown $USER /etc/flux/system/curve.cert
brokerOptions="-Stbon.fanout=256 -Slog-stderr-level=7"
flux broker --config-path /tmp/lima/broker.toml $brokerOptions
```
```console
Nov 22 02:36:40.080343 broker.debug[0]: insmod connector-local
Nov 22 02:36:40.080479 broker.info[0]: start: none->join 0.788668ms
Nov 22 02:36:40.080575 broker.info[0]: parent-none: join->init 0.068678ms
Nov 22 02:36:40.081433 connector-local.debug[0]: allow-guest-user=true
Nov 22 02:36:40.081449 connector-local.debug[0]: allow-root-owner=true
Nov 22 02:36:40.095172 broker.debug[0]: insmod content
Nov 22 02:36:40.098541 broker.debug[0]: insmod barrier
Nov 22 02:36:40.116660 broker.debug[0]: insmod content-sqlite
Nov 22 02:36:40.119567 content-sqlite.debug[0]: /tmp/flux-EwoRJc/content.sqlite (0 objects) journal_mode=OFF synchronous=OFF
Nov 22 02:36:40.120575 content.debug[0]: content backing store: enabled content-sqlite
Nov 22 02:36:40.128957 broker.debug[0]: insmod kvs
Nov 22 02:36:40.147420 broker.debug[0]: insmod kvs-watch
Nov 22 02:36:40.182680 broker.debug[0]: insmod resource
Nov 22 02:36:40.198714 resource.debug[0]: reslog_cb: resource-init event posted
Nov 22 02:36:40.203514 broker.debug[0]: insmod cron
Nov 22 02:36:40.203852 cron.info[0]: synchronizing cron tasks to event heartbeat.pulse
Nov 22 02:36:40.210528 broker.debug[0]: insmod job-manager
Nov 22 02:36:40.210987 job-manager.debug[0]: jobtap plugin .history registered method job-manager.history.get
Nov 22 02:36:40.211196 job-manager.info[0]: restart: 0 jobs
Nov 22 02:36:40.211203 job-manager.info[0]: restart: 0 running jobs
Nov 22 02:36:40.211344 job-manager.info[0]: restart: checkpoint.job-manager not found
Nov 22 02:36:40.211356 job-manager.debug[0]: restart: max_jobid=Æ’1
Nov 22 02:36:40.227538 broker.debug[0]: insmod job-info
Nov 22 02:36:40.246082 broker.debug[0]: insmod job-list
Nov 22 02:36:40.246701 job-list.debug[0]: job_state_init_from_kvs: read 0 jobs
Nov 22 02:36:40.290943 broker.debug[0]: insmod job-ingest
Nov 22 02:36:40.291381 job-ingest.debug[0]: configuring validator with plugins=(null), args=(null) (enabled)
Nov 22 02:36:40.291685 job-ingest.debug[0]: fluid ts=1ms
Nov 22 02:36:40.304415 broker.debug[0]: insmod job-exec
Nov 22 02:36:40.305177 job-exec.debug[0]: using default shell path /usr/libexec/flux/flux-shell
Nov 22 02:36:40.305199 job-exec.debug[0]: using imp path /usr/libexec/flux/flux-imp (with helper)
Nov 22 02:36:40.312364 broker.debug[0]: insmod heartbeat
Nov 22 02:36:40.313178 broker.info[0]: rc1.0: running /etc/flux/rc1.d/01-sched-fluxion
Nov 22 02:36:40.324007 broker.debug[0]: insmod sched-fluxion-resource
Nov 22 02:36:40.324160 sched-fluxion-resource.info[0]: version 0.30.0
Nov 22 02:36:40.324219 sched-fluxion-resource.debug[0]: mod_main: resource module starting
Nov 22 02:36:40.327597 broker.debug[0]: insmod sched-fluxion-qmanager
Nov 22 02:36:40.327783 sched-fluxion-qmanager.info[0]: version 0.30.0
Nov 22 02:36:40.327839 sched-fluxion-qmanager.debug[0]: service_register
Nov 22 02:36:40.327861 sched-fluxion-qmanager.debug[0]: enforced policy (queue=default): fcfs
Nov 22 02:36:40.327868 sched-fluxion-qmanager.debug[0]: effective queue params (queue=default): default
Nov 22 02:36:40.327871 sched-fluxion-qmanager.debug[0]: effective policy params (queue=default): default
Nov 22 02:36:40.328220 broker.info[0]: rc1.0: running /etc/flux/rc1.d/02-cron
Nov 22 02:36:40.405501 broker.info[0]: rc1.0: /etc/flux/rc1 Exited (rc=0) 0.3s
Nov 22 02:36:40.405618 broker.info[0]: rc1-success: init->quorum 0.325012s
Nov 22 02:36:40.506174 broker.debug[0]: groups: broker.online=0
Nov 22 02:36:40.507277 broker.info[0]: online: lima-flux-0 (ranks 0)
Nov 22 02:36:40.509453 resource.debug[0]: reslog_cb: online event posted
```


Now on the flux-1 (first worker node) we do the same to connect.

#### Worker Broker

Then shell into the flux-1

```bash
limactl shell --workdir /home/vanessa.linux/usernetes flux-1
```

And start the follower broker:

```bash
sudo chown $USER /etc/flux/system/curve.cert
brokerOptions="-Stbon.fanout=256 -Slog-stderr-level=7"
flux broker --config-path /tmp/lima/broker.toml $brokerOptions
```

When it connects, the lead broker will appear to exit, but you are actually in the instance
and can get the URI.

```
$ echo $FLUX_URI
local:///tmp/flux-u3C8of/local-0
```

#### Usernetes

Finally, shell into the lead broker and connect to the running flux instance:

```bash
limactl shell --workdir /home/vanessa.linux/usernetes flux-0
```

You should see two nodes:

```bash
$ flux proxy local:///tmp/flux-u3C8of/local-0 bash
vanessa@lima-flux-0:~/usernetes$ flux resource list
     STATE NNODES   NCORES    NGPUS NODELIST
      free      2        8        0 lima-flux-[0-1]
 allocated      0        0        0 
      down      0        0        0 
```

Note that we are on the lead broker flux-0 which also is the control plane for usernetes.
Try exporting KUBECONFIG and then seeing if it's still working...

```bash
export KUBECONFIG=$HOME/usernetes/kubeconfig
```
```
kubectl  get nodes
NAME              STATUS   ROLES           AGE   VERSION
u7s-lima-flux-0   Ready    control-plane   90m   v1.28.0
u7s-lima-flux-1   Ready    <none>          61m   v1.28.0
```

This is great! We now know that (with manual setup) we can run usernetes inside of a flux instance.
Now we need to put these two steps together so a batch job can both setup Usernetes and still get a working
instance.

```bash
$ kubectl  get pods -n kube-system
```
```console
NAME                                      READY   STATUS    RESTARTS   AGE
coredns-5dd5756b68-7cftg                  1/1     Running   0          91m
coredns-5dd5756b68-9q8vs                  1/1     Running   0          91m
etcd-u7s-lima-flux-0                      1/1     Running   0          91m
kube-apiserver-u7s-lima-flux-0            1/1     Running   0          91m
kube-controller-manager-u7s-lima-flux-0   1/1     Running   0          91m
kube-proxy-48ndz                          1/1     Running   0          91m
kube-proxy-sml2j                          1/1     Running   0          63m
kube-scheduler-u7s-lima-flux-0            1/1     Running   0          91m
```

## Clean Up

You can stop:

```bash
limactl stop control-plane
limactl stop usernetes-worker
```

I haven't played around with restarting - likely services would need to be restarted, etc.
If you come back:

```bash
limactl start --network=lima:user-v2 control-plane
limactl start --network=lima:user-v2 usernetes-worker
```

or just nuke it!

```bash
limactl delete control-plane
limactl delete usernetes-worker
```
